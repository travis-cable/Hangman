{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Initial packages\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "from einops import reduce\n",
    "\n",
    "# ML packages for neural-network\n",
    "import torch as T\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch import einsum\n",
    "import gymnasium as gym\n",
    "\n",
    "# T.autograd.set_detect_anomaly(True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def load_dictionary(file_path):\n",
    "\n",
    "    # Read in the .txt file\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = file.readlines()\n",
    "\n",
    "    # Remove newline characters and whitespace\n",
    "    words_list = []\n",
    "    for word in words:\n",
    "        temp_word = word.strip()\n",
    "        if len(temp_word) > 1:\n",
    "            words_list.append(temp_word)\n",
    "\n",
    "    trimmed_words_list = [word for word in words_list if (len(word) == 9)]\n",
    "\n",
    "    # Randomly shuffle the list\n",
    "    random.seed(2024)\n",
    "    random.shuffle(trimmed_words_list)\n",
    "\n",
    "    return trimmed_words_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Masking Function provided through: https://boring-guy.sh/posts/masking-rl/\n",
    "\n",
    "class CategoricalMasked(Categorical):\n",
    "    def __init__(self, logits: T.Tensor, mask: Optional[T.Tensor] = None):\n",
    "        self.mask = mask\n",
    "        self.batch, self.nb_action = logits.size()\n",
    "        if mask is None:\n",
    "            super(CategoricalMasked, self).__init__(logits=logits)\n",
    "        else:\n",
    "            self.mask_value = T.tensor(\n",
    "                T.finfo(logits.dtype).min, dtype=logits.dtype\n",
    "            )\n",
    "            logits = T.where(self.mask, logits, self.mask_value)\n",
    "            super(CategoricalMasked, self).__init__(logits=logits)\n",
    "\n",
    "    def entropy(self):\n",
    "        if self.mask is None:\n",
    "            return super().entropy()\n",
    "        # Elementwise multiplication\n",
    "        p_log_p = einsum(\"ij,ij->ij\", self.logits, self.probs)\n",
    "        # Compute the entropy with possible action only\n",
    "        p_log_p = T.where(\n",
    "            self.mask,\n",
    "            p_log_p,\n",
    "            T.tensor(0, dtype=p_log_p.dtype, device=p_log_p.device),\n",
    "        )\n",
    "        return -reduce(p_log_p, \"b a -> b\", \"sum\", b=self.batch, a=self.nb_action)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class HangmanEnv(gym.Env):\n",
    "    def __init__(self, dictionary, total_lives):\n",
    "        super(HangmanEnv, self).__init__()\n",
    "\n",
    "        # Lives for the game\n",
    "        self.total_lives = total_lives\n",
    "\n",
    "        # The dictionary will not change, so save it here\n",
    "        self.dictionary = dictionary\n",
    "        self.valid_indices = [index for index in range(len(self.dictionary))]\n",
    "        self.incorrect_indices = []\n",
    "\n",
    "        # Action space involves choosing ['a','b',...,'y','z'] --> [0,1,...,24,25]\n",
    "        self.action_space = gym.spaces.Discrete(26)\n",
    "\n",
    "        # Observation (i.e. state space) the one-hot encoding of the current word along with the information about currently guessed letters\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(837,), dtype=np.int8)\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        # Turn the action into a letter\n",
    "        current_guess = chr(action + ord('a'))\n",
    "\n",
    "        self.guessed_letters.append(current_guess)\n",
    "        self.available_letters.remove(current_guess)\n",
    "\n",
    "        # If the current guess is in the word, append the guess to the word state\n",
    "        if current_guess in self.guess_word:\n",
    "            correct_indices = np.where(self.guess_word == current_guess)[0]\n",
    "            self.current_word_state[correct_indices] = current_guess\n",
    "            self.reward = len(correct_indices) / len(self.guess_word)\n",
    "            # self.reward = 0\n",
    "\n",
    "        # Not in the word, then remove a life\n",
    "        else:\n",
    "            correct_indices = []\n",
    "            self.lives_remaining -= 1\n",
    "            self.reward = 0\n",
    "\n",
    "        # Create the current state vector\n",
    "        if str(self.current_word_state) == str(self.guess_word):\n",
    "            self.status = 1\n",
    "            self.reward = self.lives_remaining * 10\n",
    "            self.done = True\n",
    "        elif self.lives_remaining == 0:\n",
    "            self.status = 0\n",
    "            self.done = True\n",
    "\n",
    "        # Update the current letter guessed\n",
    "        self.one_hot_state[-1, action] = -1\n",
    "        for index in correct_indices:\n",
    "            self.one_hot_state[index, action] = 1\n",
    "            self.one_hot_state[index, -1] = 0\n",
    "        self.observation = self.one_hot_state.flatten()\n",
    "\n",
    "        info = {}\n",
    "        truncated = False\n",
    "        return self.observation, self.reward, self.done, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        # Initialize our environment here\n",
    "        self.lives_remaining = self.total_lives\n",
    "        self.available_letters = list(string.ascii_lowercase)\n",
    "        self.guessed_letters = []\n",
    "        self.done = False\n",
    "        self.status = 0\n",
    "        self.reward = 0\n",
    "\n",
    "        # Now draw a word from either the total or incorrect word replay dictionaries\n",
    "        if len(self.incorrect_indices) > 0:\n",
    "            dict_choice = np.random.choice([0,1])\n",
    "\n",
    "            # If 0, sample from the unsearched words in the full dictionary\n",
    "            if dict_choice == 0:\n",
    "                self.guess_index = np.random.choice(self.valid_indices)\n",
    "            else:\n",
    "                self.guess_index = np.random.choice(self.incorrect_indices)\n",
    "        else:\n",
    "            self.guess_index = np.random.choice(self.valid_indices)\n",
    "\n",
    "        self.guess_word = np.array(list(self.dictionary[self.guess_index]))\n",
    "        self.current_word_state = np.array(['_'] * len(self.guess_word))\n",
    "\n",
    "        # Initialize the current state vector\n",
    "        self.one_hot_state = np.zeros([31, 27], dtype=np.int8)\n",
    "        for i in range(len(self.guess_word)):\n",
    "            self.one_hot_state[i, -1] = -1\n",
    "            self.one_hot_state[-1, -1] = -1\n",
    "        self.observation = self.one_hot_state.flatten()\n",
    "\n",
    "        info = {}\n",
    "        return self.observation, info\n",
    "\n",
    "    def render(self, action):\n",
    "\n",
    "        result = ' '.join([str(elem) for elem in self.current_word_state])\n",
    "\n",
    "        print(f'Guessed {chr(action + ord(\"a\"))}. The current word state is {result} and with {self.lives_remaining} lives remaining.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# PPO classes originally defined here: https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning/PolicyGradient/PPO/torch\n",
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, alpha,\n",
    "            fc1_dims=256, fc2_dims=256, chkpt_dir='tmp/ppo'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        self.actor = nn.Sequential(\n",
    "                nn.Linear(*input_dims, fc1_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc2_dims, n_actions),\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "\n",
    "        return dist\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\n",
    "            chkpt_dir='tmp/ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dims, fc1_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc2_dims, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
    "            policy_clip=0.1, batch_size=64, n_epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "\n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, observation, viable_actions):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "\n",
    "        # Calculate distribution over actions via the actor network\n",
    "        dist = self.actor(state)\n",
    "\n",
    "        # Modify the categorical distribution based on the available actions\n",
    "        chosen_letters_indices = np.where(viable_actions == -1)[0]\n",
    "        if len(chosen_letters_indices) > 0:\n",
    "            mask = T.ones(dist.logits.shape, dtype=T.bool) # batch size, nb action\n",
    "            mask[0, chosen_letters_indices] = False\n",
    "            masked_dist = CategoricalMasked(logits=dist.logits, mask=mask)\n",
    "\n",
    "            # Sample the conditional distribution\n",
    "            action = masked_dist.sample()\n",
    "            probs = T.squeeze(masked_dist.log_prob(action)).item()\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "            probs = T.squeeze(dist.log_prob(action)).item()\n",
    "\n",
    "        # Determine the value of the guesses state via the critic network\n",
    "        value = self.critic(state)\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        action = T.squeeze(action).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                #prob_ratio = (new_probs - old_probs).exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in dictionary with 30906 words.\n"
     ]
    }
   ],
   "source": [
    "dictionary_path = 'words_250000_train.txt'\n",
    "words_list = load_dictionary(dictionary_path)\n",
    "\n",
    "print(f'Loaded in dictionary with {len(words_list)} words.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = HangmanEnv(dictionary=words_list, total_lives=6)\n",
    "N = 2048\n",
    "batch_size = 64\n",
    "epochs = 4\n",
    "alpha = 0.0003\n",
    "agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, alpha=alpha, n_epochs=epochs, input_dims=env.observation_space.shape)\n",
    "# agent.actor.load_checkpoint()\n",
    "# agent.critic.load_checkpoint()\n",
    "\n",
    "n_games = len(env.dictionary) * 2\n",
    "n_steps = 0\n",
    "learn_iters = 0\n",
    "wins_list = []\n",
    "\n",
    "progress_bar = tqdm(range(n_games), desc='Sweeping through all words', leave=False)\n",
    "for i in progress_bar:\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action, prob, val = agent.choose_action(observation, env.one_hot_state[-1, :-1])\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        # env.render(action)\n",
    "        if (n_steps+1) % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        observation = observation_\n",
    "\n",
    "    # Update the word index lists\n",
    "    if env.guess_index in env.incorrect_indices:\n",
    "       if env.status == 1:\n",
    "           env.incorrect_indices.remove(env.guess_index)\n",
    "    else:\n",
    "        env.valid_indices.remove(env.guess_index)\n",
    "        if env.status == 0:\n",
    "            env.incorrect_indices.append(env.guess_index)\n",
    "\n",
    "    # Tabulate wins and win rate\n",
    "    wins_list.append(env.status)\n",
    "    win_rate = np.round(np.mean(wins_list[-1000:]), 4) * 100\n",
    "\n",
    "    progress_bar.set_postfix_str(s=f'Win Rate/1000: {win_rate}%.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "agent.actor.save_checkpoint()\n",
    "agent.critic.save_checkpoint()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
