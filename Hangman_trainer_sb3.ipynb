{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Initial packages\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ML packages for neural-network\n",
    "import torch as T\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN, A2C, PPO\n",
    "from stable_baselines3.common.policies import obs_as_tensor\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def load_dictionary(file_path):\n",
    "\n",
    "    # Read in the .txt file\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = file.readlines()\n",
    "\n",
    "    # Remove newline characters and whitespace\n",
    "    words_list = []\n",
    "    for word in words:\n",
    "        temp_word = word.strip()\n",
    "        if len(temp_word) > 1:\n",
    "            words_list.append(temp_word)\n",
    "\n",
    "    trimmed_words_list = [word for word in words_list if (len(word) > 3)]\n",
    "\n",
    "    # Randomly shuffle the list\n",
    "    # random.seed(2024)\n",
    "    random.shuffle(trimmed_words_list)\n",
    "\n",
    "    return trimmed_words_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "dictionary_path = 'words_250000_train.txt'\n",
    "words_list = load_dictionary(dictionary_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class HangmanEnv(gym.Env):\n",
    "    def __init__(self, dictionary, total_lives):\n",
    "        super(HangmanEnv, self).__init__()\n",
    "\n",
    "        # Number of lives per game\n",
    "        self.total_lives = total_lives\n",
    "\n",
    "        # The dictionary will not change, so save it here\n",
    "        self.dictionary = dictionary\n",
    "        self.incorrect_words = []\n",
    "\n",
    "        # Action space involves choosing ['a','b',...,'y','z'] --> [0,1,...,24,25]\n",
    "        self.action_space = gym.spaces.Discrete(26)\n",
    "\n",
    "        # Observation (i.e. state space) the one-hot encoding of the current word along with the information about currently guessed letters\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(837,), dtype=np.int8)\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        # Turn the action into a letter\n",
    "        current_guess = chr(action + ord('a'))\n",
    "\n",
    "        if current_guess not in self.guessed_letters:\n",
    "            self.guessed_letters.append(current_guess)\n",
    "            self.available_letters.remove(current_guess)\n",
    "        else:\n",
    "            self.observation = self.one_hot_state.flatten()\n",
    "            self.reward = -0.1\n",
    "            self.done = False\n",
    "            truncated = False\n",
    "            info = {}\n",
    "            return self.observation, self.reward, self.done, truncated, info\n",
    "\n",
    "        # If the current guess is in the word, append the guess to the word state\n",
    "        if current_guess in self.guess_word:\n",
    "            correct_indices = np.where(self.guess_word == current_guess)[0]\n",
    "            self.current_word_state[correct_indices] = current_guess\n",
    "            # self.reward = len(correct_indices) / len(self.guess_word)\n",
    "            self.reward = 0\n",
    "\n",
    "        # Not in the word, then remove a life\n",
    "        else:\n",
    "            correct_indices = []\n",
    "            self.lives_remaining -= 1\n",
    "            self.reward = 0\n",
    "\n",
    "        # Create the current state vector\n",
    "        if str(self.current_word_state) == str(self.guess_word):\n",
    "            self.status = 1\n",
    "            self.reward = self.lives_remaining * 10\n",
    "            self.done = True\n",
    "        elif self.lives_remaining == 0:\n",
    "            self.status = 0\n",
    "            self.done = True\n",
    "\n",
    "        # Update the current letter guessed\n",
    "        self.one_hot_state[-1, action] = -1\n",
    "        for index in correct_indices:\n",
    "            self.one_hot_state[index, action] = 1\n",
    "            self.one_hot_state[index, -1] = 0\n",
    "        self.observation = self.one_hot_state.flatten()\n",
    "\n",
    "        info = {}\n",
    "        truncated = False\n",
    "        return self.observation, self.reward, self.done, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        # Initialize our environment here\n",
    "        self.lives_remaining = self.total_lives\n",
    "        self.available_letters = list(string.ascii_lowercase)\n",
    "        self.guessed_letters = []\n",
    "        self.done = False\n",
    "        self.status = 0\n",
    "\n",
    "        # Now draw a word from the dictionary\n",
    "        guess_index = np.random.choice(len(self.dictionary))\n",
    "        self.guess_word = np.array(list(self.dictionary[guess_index]))\n",
    "        self.current_word_state = np.array(['_'] * len(self.guess_word))\n",
    "\n",
    "        # Initialize the current state vector\n",
    "        self.one_hot_state = np.zeros([31, 27], dtype=np.int8)\n",
    "        for i in range(len(self.guess_word)):\n",
    "            self.one_hot_state[i, -1] = -1\n",
    "            self.one_hot_state[-1, -1] = -1\n",
    "        self.observation = self.one_hot_state.flatten()\n",
    "\n",
    "        info = {}\n",
    "        return self.observation, info\n",
    "\n",
    "    def render(self):\n",
    "\n",
    "        result = ' '.join([str(elem) for elem in self.current_word_state])\n",
    "\n",
    "        print(f'The current word state is {result} and with {self.lives_remaining} lives remaining.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "models_dir = \"models/PPO\"\n",
    "logdir = \"log\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "env = HangmanEnv(dictionary=words_list, total_lives=6)\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=logdir)\n",
    "timesteps = 100000\n",
    "for i in range(1,101):\n",
    "    model.learn(total_timesteps=timesteps, reset_num_timesteps=False, tb_log_name=\"PPO\")\n",
    "    model.save(f'{models_dir}/{timesteps*i}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Playing Hangman Games:  30%|███       | 3/10 [00:23<00:54,  7.74s/it, Win Rate/1000: 0.0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m---> 13\u001B[0m     action, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     obs, reward, done, truncated, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m game \u001B[38;5;241m>\u001B[39m (total_games \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m5\u001B[39m):\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/trexquant/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:556\u001B[0m, in \u001B[0;36mBaseAlgorithm.predict\u001B[0;34m(self, observation, state, episode_start, deterministic)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    538\u001B[0m     observation: Union[np\u001B[38;5;241m.\u001B[39mndarray, Dict[\u001B[38;5;28mstr\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray]],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    541\u001B[0m     deterministic: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    542\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[np\u001B[38;5;241m.\u001B[39mndarray, Optional[Tuple[np\u001B[38;5;241m.\u001B[39mndarray, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]]]:\n\u001B[1;32m    543\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    544\u001B[0m \u001B[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001B[39;00m\n\u001B[1;32m    545\u001B[0m \u001B[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    554\u001B[0m \u001B[38;5;124;03m        (used in recurrent policies)\u001B[39;00m\n\u001B[1;32m    555\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 556\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisode_start\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/trexquant/lib/python3.12/site-packages/stable_baselines3/common/policies.py:368\u001B[0m, in \u001B[0;36mBasePolicy.predict\u001B[0;34m(self, observation, state, episode_start, deterministic)\u001B[0m\n\u001B[1;32m    365\u001B[0m obs_tensor, vectorized_env \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobs_to_tensor(observation)\n\u001B[1;32m    367\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 368\u001B[0m     actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdeterministic\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001B[39;00m\n\u001B[1;32m    370\u001B[0m actions \u001B[38;5;241m=\u001B[39m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mreshape((\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39mshape))  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/trexquant/lib/python3.12/site-packages/stable_baselines3/common/policies.py:717\u001B[0m, in \u001B[0;36mActorCriticPolicy._predict\u001B[0;34m(self, observation, deterministic)\u001B[0m\n\u001B[1;32m    709\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_predict\u001B[39m(\u001B[38;5;28mself\u001B[39m, observation: PyTorchObs, deterministic: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m th\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m    710\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    711\u001B[0m \u001B[38;5;124;03m    Get the action according to the policy for a given observation.\u001B[39;00m\n\u001B[1;32m    712\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    715\u001B[0m \u001B[38;5;124;03m    :return: Taken action according to the policy\u001B[39;00m\n\u001B[1;32m    716\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 717\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mget_actions(deterministic\u001B[38;5;241m=\u001B[39mdeterministic)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/trexquant/lib/python3.12/site-packages/stable_baselines3/common/policies.py:752\u001B[0m, in \u001B[0;36mActorCriticPolicy.get_distribution\u001B[0;34m(self, obs)\u001B[0m\n\u001B[1;32m    750\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mextract_features(obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpi_features_extractor)\n\u001B[1;32m    751\u001B[0m latent_pi \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp_extractor\u001B[38;5;241m.\u001B[39mforward_actor(features)\n\u001B[0;32m--> 752\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_action_dist_from_latent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlatent_pi\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/trexquant/lib/python3.12/site-packages/stable_baselines3/common/policies.py:691\u001B[0m, in \u001B[0;36mActorCriticPolicy._get_action_dist_from_latent\u001B[0;34m(self, latent_pi)\u001B[0m\n\u001B[1;32m    684\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_action_dist_from_latent\u001B[39m(\u001B[38;5;28mself\u001B[39m, latent_pi: th\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Distribution:\n\u001B[1;32m    685\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    686\u001B[0m \u001B[38;5;124;03m    Retrieve action distribution given the latent codes.\u001B[39;00m\n\u001B[1;32m    687\u001B[0m \n\u001B[1;32m    688\u001B[0m \u001B[38;5;124;03m    :param latent_pi: Latent code for the actor\u001B[39;00m\n\u001B[1;32m    689\u001B[0m \u001B[38;5;124;03m    :return: Action distribution\u001B[39;00m\n\u001B[1;32m    690\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 691\u001B[0m     mean_actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlatent_pi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    693\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dist, DiagGaussianDistribution):\n\u001B[1;32m    694\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dist\u001B[38;5;241m.\u001B[39mproba_distribution(mean_actions, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_std)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/trexquant/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/trexquant/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/trexquant/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = PPO.load(\"models/PPO/25000000.zip\")\n",
    "\n",
    "env = HangmanEnv(dictionary=words_list, total_lives=6)\n",
    "\n",
    "total_games = 10\n",
    "wins_list = []\n",
    "progress_bar = tqdm(range(total_games), desc='Playing Hangman Games')\n",
    "for game in progress_bar:\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        if game > (total_games - 5):\n",
    "            env.render()\n",
    "\n",
    "    wins_list.append(env.status)\n",
    "    progress_bar.set_postfix_str(s=f'Win Rate/1000: {np.round(np.mean(wins_list), 3)}')\n",
    "\n",
    "    if game > (total_games - 5):\n",
    "        print('=================================')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
